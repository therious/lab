# Definition Similarity Grading Methodology

## Overview

This document explains how the definition similarity grades in `definition-similarity-grades.ts` were generated. The grading was performed using a **rule-based pattern matching approach** implemented in `generate-similarity-grades.ts`, not through direct AI semantic analysis of the full meaning of each definition.

## Methodology

### Approach Used

The similarity grades were generated by running the script `generate-similarity-grades.ts`, which uses a **multi-layered pattern matching algorithm** rather than true semantic understanding. The algorithm works as follows:

1. **Exact/Word Matching**: Compares definitions for exact word matches
2. **Concept Extraction**: Uses a predefined list of ~50 semantic concepts with keyword patterns
3. **Concept Grouping**: Checks if extracted concepts belong to related concept groups
4. **Stem Matching**: Performs simple prefix matching (first 4 characters) for word similarity

### Grading Logic

The `gradeDefinitions()` function assigns grades using this hierarchy:

- **Grade 5**: Exact or near-exact definition match
- **Grade 4**: 
  - 70%+ word overlap, OR
  - 2+ shared semantic concepts
- **Grade 3**: 1 shared concept + at least 1 common word
- **Grade 2**: 1 shared concept OR related concepts from the same group
- **Grade 1**: At least 1 common word OR stem-based word similarity
- **Grade 0**: No strong relation (not stored in the data file)

### Concept Extraction

The `extractConcepts()` function uses **hardcoded keyword patterns** to identify concepts. For example:
- `'collect'` concept: matches "collect", "gather", "store"
- `'pain'` concept: matches "pain", "distress", "suffer"
- `'connect'` concept: matches "connect", "tie", "bind", "combine"

**Origin of the Concepts**: The ~50 predefined concepts were created **ad-hoc by the AI** when writing the script. They were derived by:
1. Scanning through a sample of root definitions
2. Identifying common semantic themes that appeared frequently
3. Creating keyword patterns for those themes

**This was not a systematic analysis** - the concepts were chosen based on what seemed common in a quick review of the definitions, not from:
- A comprehensive analysis of all definitions
- An external semantic database or ontology
- A statistical analysis of word frequencies
- Any linguistic or semantic theory

**Important Limitation**: Only ~50 concepts are defined. If a word doesn't match any of these patterns, it won't be recognized as a concept, even if it's semantically important. Many important concepts are missing because they weren't identified during the initial ad-hoc review.

### Example: The "Surprise" Case

You noted that two definitions both containing "surprise" appeared to have no connection:
- Root 1449: `'surprise; be unprepared'`
- Root 1455: `'surprise; act without concern for consequence'`

**What actually happened**: 
- These two roots **did** receive a grade of **1** (stored as `[1449, 1455, 1]`)
- However, this was only through **simple word matching** - both definitions contain the exact word "surprise"
- "surprise" is **not** in the predefined concept list in `extractConcepts()`, so it wasn't recognized as a semantic concept
- Because there were no shared concepts and only one common word, it received the lowest grade (1) rather than a higher grade that would reflect the semantic relationship

**Why this is problematic**:
- The algorithm treats "surprise" as just a word match, not as a meaningful semantic concept
- Two definitions that both center around "surprise" should arguably receive a grade of 2-3 (very plausible to clearly connected), not just 1 (plausible)
- This illustrates a key limitation: **the algorithm only recognizes concepts that were explicitly programmed into it**. Words like "surprise", "wonder", "amazement", "shock", etc. are not in the concept list, so semantic relationships based on these ideas receive lower grades than they should

## Limitations

### 1. **Limited Concept Coverage**
- Only ~50 semantic concepts are defined
- Many important semantic ideas are not represented
- Examples of missing concepts: surprise, wonder, time-related concepts (past/future), spatial concepts (above/below), emotional states beyond pain/anger/love

### 2. **No True Semantic Understanding**
- The algorithm does not understand meaning, context, or nuance
- It cannot recognize synonyms that aren't explicitly listed
- It cannot understand metaphorical or abstract connections
- It cannot distinguish between different senses of the same word

### 3. **Pattern Matching Only**
- Relies on exact string matching and simple prefix matching
- Cannot handle:
  - Different word forms (surprise vs. surprising vs. surprised)
  - Related but different words (surprise vs. shock vs. astonishment)
  - Context-dependent meanings

### 4. **No Cross-Language or Cultural Understanding**
- Cannot understand Hebrew-specific semantic nuances
- Cannot recognize culturally-specific concept relationships
- Treats all definitions as English text only

### 5. **Binary Concept Matching**
- Concepts are either present or absent (no degrees)
- Cannot represent partial or contextual concept presence
- Related concepts are grouped, but the groupings are manually defined and may miss relationships

## What Was Actually Used

**Yes, the `generate-similarity-grades.ts` script is what was actually used** to generate the grades. This is a rule-based pattern matching system, not a true AI semantic analysis system.

The script was run once to generate all 53,743 similarity grades, and those results were stored in `definition-similarity-grades.ts`.

## Recommendations for Improvement

If you want more accurate and comprehensive similarity grades, consider:

1. **Using an AI/LLM API**: Send definition pairs to a language model API (like OpenAI, Anthropic, etc.) to get true semantic similarity scores
2. **Expanding the concept list**: Add many more concepts to the `extractConcepts()` function
3. **Using NLP libraries**: Leverage libraries like spaCy or natural for better word similarity and concept extraction
4. **Manual review**: For critical connections, manually review and adjust grades
5. **Hybrid approach**: Use the current automated system as a first pass, then use AI or manual review to catch missed connections

## Current Results

Despite the limitations, the current system did identify 53,743 pairs with some level of similarity:
- Grade 1: 19,185 pairs (plausible connections)
- Grade 2: 23,779 pairs (very plausible)
- Grade 3: 4,829 pairs (clearly connected)
- Grade 4: 5,438 pairs (definite overlap)
- Grade 5: 512 pairs (practically the same)

However, as your "surprise" example demonstrates, **many valid semantic connections are likely missing** due to the limitations of the pattern-matching approach.

## Conclusion

The similarity grades were generated using a rule-based pattern matching system, not true AI semantic analysis. While this approach can identify many connections, it has significant limitations and will miss relationships that don't match its predefined patterns. The "surprise" case you identified is a clear example of this limitation.

